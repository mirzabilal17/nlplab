{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0ca3c3-dcbf-41d1-8272-bea0a2ae2454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['children', \"shouldn't\", 'as', 'one', 'word', 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']\n",
      "children shouldn't as one word drink a sugary drink before bed.\n"
     ]
    }
   ],
   "source": [
    "#Word SpLitting \n",
    "text1=\"children shouldn't as one word drink a sugary drink before bed.\"\n",
    "print(text1.split(' '))\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52567726-774d-4ce1-b593-1db2c837d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#using nltk package--wordtokenize and sent tokenize\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c983f2e-1f81-413f-98dc-63d9b3628190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['children', 'should', \"n't\", 'as', 'one', 'word', 'drink', 'a', 'sugary', 'drink', 'before', 'bed', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d369335-1823-4b16-ad20-ac7378966200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' this is the first sentence.', 'A gallon of milk in the U.S. cost $2.99.', 'Is tt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "s=\" this is the first sentence. A gallon of milk in the U.S. cost $2.99. Is tt\"\n",
    "print(sent_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac72920b-0b25-4f6a-a580-ace9285f9c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', ' listed', ' lists', ' listing', ' listings ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NORMALIZATION\n",
    "input1=u\"List, Listed, lists, listing, listings \" \n",
    "words=input1.lower().split(',') \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae58f3f-1cf2-42f9-aaf7-c4fe839232bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', ' list', ' list', ' list', ' listings ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEMMING\n",
    "import nltk\n",
    "porter=nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d58cd1-d27d-42f9-adfb-cfbeea098d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "# corpus of the Universal DecLarat ion of Human Rights (udhr) \n",
    "udhr=nltk.corpus.udhr.words('English-Latin1') \n",
    "print(udhr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f40b479-7b01-463b-a462-fc5e457521a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['univers', 'declar', 'of', 'human', 'right', 'preambl', 'wherea', 'recognit', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "print([ porter.stem (t) for t in udhr[:10]]) \n",
    "#that univers and decLar are not real Ly valid words. \n",
    "# Lemmatization woulLd do that stemming, but rare Ly keep the resul ting stems to be \n",
    "#It is sometimes useful because you want to somehow normaLize it, \n",
    "# but normalize it to something that is also meaningfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218baa3f-debb-4731-a67f-d23504c2c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "#LEMMITAZATION\n",
    "WNlemma=nltk.WordNetLemmatizer() \n",
    "print ([ WNlemma.lemmatize(t) for t in udhr[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f87271-a417-4ff1-bd38-eeab9b2b76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('children', 'NNS'), ('should', 'MD'), (\"n't\", 'RB'), ('as', 'IN'), ('one', 'CD'), ('word', 'NN'), ('drink', 'VB'), ('a', 'DT'), ('sugary', 'JJ'), ('drink', 'NN'), ('before', 'IN'), ('bed', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#POS\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "text12=\"children shouldn't as one word drink a sugary drink before bed. \" \n",
    "tokens =word_tokenize(text12) # Tokenize the text into words \n",
    "tagged_words =nltk.pos_tag(tokens) # Perform part-of-speech tagging \n",
    "print (tagged_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "646ca282-7870-447e-9709-9d363cb02047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n"
     ]
    }
   ],
   "source": [
    " nltk.help.upenn_tagset('CD') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ba789a-fd2a-42c9-9dc6-195ef29c7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Visting', 'VBG'), ('aunts', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('a', 'DT'), ('nuisance', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text13=\"Visting aunts can be a nuisance\" \n",
    "tokens=nltk.word_tokenize(text13) \n",
    "print(nltk.pos_tag(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65db8204-8ff7-45fb-a9dc-3fa2e973f4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text18=nltk. word_tokenize(\"the old man the boat\")\n",
    "nltk.pos_tag(text18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c26b676d-a057-43fb-9b80-82d06d81d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Colorless', 'NNP'), ('green', 'JJ'), ('ideas', 'NNS'), ('sleep', 'VBP'), ('furiously', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "text19=nltk.word_tokenize(\"Colorless green ideas sleep furiously\" )  \n",
    "print(nltk. pos_tag(text19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb36c035-e126-4eda-af80-6416a11934b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> Det N\n",
      "    PP -> P NP\n",
      "    VP -> 'slept'\n",
      "    VP -> 'saw' NP\n",
      "    VP -> 'walked' PP\n",
      "    Det -> 'the'\n",
      "    Det -> 'a'\n",
      "    N -> 'man'\n",
      "    N -> 'park'\n",
      "    N -> 'dog'\n",
      "    P -> 'in'\n",
      "    P -> 'with'\n"
     ]
    }
   ],
   "source": [
    "#GRAMMER\n",
    "from nltk.parse.generate import generate, demo_grammar \n",
    "from nltk import CFG \n",
    "grammar =CFG.fromstring(demo_grammar) \n",
    "print(grammar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a0f9d5d-d550-40b1-b696-2db1974fb8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man slept\n",
      "the man saw the man\n",
      "the man saw the park\n",
      "the man saw the dog\n",
      "the man saw a man\n",
      "the man saw a park\n",
      "the man saw a dog\n",
      "the man walked in the man\n",
      "the man walked in the park\n",
      "the man walked in the dog\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate (grammar, n=10): \n",
    "    print (' '.join(sentence ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ae4b5fb-d600-4315-a376-7a93e532cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man slept\n",
      "the park slept\n",
      "the dog slept\n",
      "a man slept\n",
      "a park slept\n",
      "a dog slept\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate (grammar, depth=4):\n",
    "    print (' '.join(sentence ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d01726bb-e596-4e05-9266-ab623dd866f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in generate(grammar, depth=2) :\n",
    "    print (' '.join(sentence ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ca69bf2-51d4-4b99-808b-c718d5d3e4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list (generate (grammar, depth=3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f23af49-b8a4-4d53-a138-1f39ea83fd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list (generate (grammar, depth=4)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec16b931-d370-4bb4-884c-f21646a62d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list (generate (grammar, depth=5)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d11a1ccc-99d1-42cd-b756-ae539e01548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list (generate (grammar, depth=6)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ca0d2fc-bf3d-4738-bdad-f23088c30556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'loves', 'bob']\n"
     ]
    }
   ],
   "source": [
    "#pasrsing sentence\n",
    "text14=\"Alice loves bob\" \n",
    "tokens=nltk.word_tokenize (text14) \n",
    "print (tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14dcd4ec-8405-4573-b76b-817c6a50e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence cannot be parsed.\n"
     ]
    }
   ],
   "source": [
    "text14=\"Alice loves Bob\" \n",
    "text20=\"Bob Alice\" \n",
    "tokens=nltk. word_tokenize (text20) \n",
    "grammar = nltk.CFG.fromstring (\"\"\"\n",
    "S -> NP VP \n",
    "NP -> \"Alice\" | \"Bob\" \n",
    "VP -> \"loves\" NP \n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammar) \n",
    "trees=parser.parse_all(tokens) \n",
    "if not trees: \n",
    "    print (\" Sentence cannot be parsed.\") \n",
    "else: \n",
    "    for tree in trees: \n",
    "        print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3caff345-0dc7-4de9-90c3-036bd1e9ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence ios not grammatically correct\n"
     ]
    }
   ],
   "source": [
    "def check_grammar_coverage (grammar, tokens) : \n",
    "    \"\"\"Check if all tokens are covered by the grammar. \n",
    "    Args: -\n",
    "    grammar: nltk.CFG object representing the context-free grammar - tokens: list of tokens (words) to be checked \n",
    "    Returns: \n",
    "    - bool: True if all tokens are covered by the grammar, False otherwi se \n",
    "    \"\"\"\n",
    "    covered_tokens = set() \n",
    "    for production in grammar. productions (): \n",
    "        for rhs in production.rhs(): \n",
    "            if isinstance (rhs, str): \n",
    "                covered_tokens.add(rhs) \n",
    "    return all(token in covered_tokens for token in tokens)\n",
    "text14 = \"Alice Bob\" \n",
    "tokens = nltk.word_tokenize(text14) \n",
    "grammar=nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP \n",
    "NP -> \"Alice\" | \"Bob\" \n",
    "VP -> \"loves\" NP \n",
    "\"\"\")\n",
    "if not check_grammar_coverage(grammar,tokens):\n",
    "    print(\"sentence is not grammmatically correct.\")\n",
    "else:\n",
    "    parser=nltk.ChartParser(grammar)\n",
    "    trees=parser.parse_all(tokens)\n",
    "    if not trees:\n",
    "        print(\"sentence ios not grammatically correct\")\n",
    "    else:\n",
    "        for tree in trees:\n",
    "            print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c00da-bff1-49ce-abc0-9dcc9308662b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
