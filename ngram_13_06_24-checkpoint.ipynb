{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter sentence:I am happy because i am learning\n",
      "uni for given text is\n",
      "[['I'], ['am'], ['happy'], ['because'], ['i'], ['am'], ['learning']]\n",
      "\n",
      "\n",
      "bi for given text is\n",
      "[['I', 'am'], ['am', 'happy'], ['happy', 'because'], ['because', 'i'], ['i', 'am'], ['am', 'learning']]\n",
      "\n",
      "\n",
      "tri for given text is\n",
      "[['I', 'am', 'happy'], ['am', 'happy', 'because'], ['happy', 'because', 'i'], ['because', 'i', 'am'], ['i', 'am', 'learning']]\n",
      "\n",
      "\n",
      "four for given text is\n",
      "[['I', 'am', 'happy', 'because'], ['am', 'happy', 'because', 'i'], ['happy', 'because', 'i', 'am'], ['because', 'i', 'am', 'learning']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ngram_model(corpus,n):\n",
    "    words=sentence.split()\n",
    "    output=[]\n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "sentence=input('enter sentence:')\n",
    "print('uni for given text is')\n",
    "n_gram=ngram_model(sentence,1)\n",
    "print(n_gram)\n",
    "print('\\n')\n",
    "\n",
    "print('bi for given text is')\n",
    "n_gram=ngram_model(sentence,2)\n",
    "print(n_gram)\n",
    "print('\\n')\n",
    "\n",
    "print('tri for given text is')\n",
    "n_gram=ngram_model(sentence,3)\n",
    "print(n_gram)\n",
    "print('\\n')\n",
    "\n",
    "print('four for given text is')\n",
    "n_gram=ngram_model(sentence,4)\n",
    "print(n_gram)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eneter sentence:am happy because i am learning\n",
      "uni gram generated is:\n",
      "('am',)\n",
      "('happy',)\n",
      "('because',)\n",
      "('i',)\n",
      "('am',)\n",
      "('learning',)\n",
      "bi gram generated is:\n",
      "('am', 'happy')\n",
      "('happy', 'because')\n",
      "('because', 'i')\n",
      "('i', 'am')\n",
      "('am', 'learning')\n",
      "tri gram generated is:\n",
      "('am', 'happy', 'because')\n",
      "('happy', 'because', 'i')\n",
      "('because', 'i', 'am')\n",
      "('i', 'am', 'learning')\n",
      "four gram generated is:\n",
      "('am', 'happy', 'because', 'i')\n",
      "('happy', 'because', 'i', 'am')\n",
      "('because', 'i', 'am', 'learning')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence=input('eneter sentence:')\n",
    "uni_grams=ngrams(sentence.split(),1)\n",
    "print('uni gram generated is:')\n",
    "for grams in uni_grams:\n",
    "    print(grams)\n",
    "\n",
    "    \n",
    "bi_grams=ngrams(sentence.split(),2)\n",
    "print('bi gram generated is:')\n",
    "for grams in bi_grams:\n",
    "    print(grams)\n",
    "\n",
    "    \n",
    "    \n",
    "tri_grams=ngrams(sentence.split(),3)\n",
    "print('tri gram generated is:')\n",
    "for grams in tri_grams:\n",
    "    print(grams)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "four_grams=ngrams(sentence.split(),4)\n",
    "print('four gram generated is:')\n",
    "for grams in four_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter sentence:i am happy because i am learning\n",
      "uni\n",
      "<generator object ngrams at 0x000002096579CC00>\n",
      "('i',) --> 2\n",
      "('am',) --> 2\n",
      "('happy',) --> 1\n",
      "('because',) --> 1\n",
      "('learning',) --> 1\n",
      "\n",
      "\n",
      "bi\n",
      "<generator object ngrams at 0x000002096579CC78>\n",
      "('i', 'am') --> 2\n",
      "('am', 'happy') --> 1\n",
      "('happy', 'because') --> 1\n",
      "('because', 'i') --> 1\n",
      "('am', 'learning') --> 1\n",
      "\n",
      "\n",
      "tri\n",
      "<generator object ngrams at 0x000002096579CC00>\n",
      "('i', 'am', 'happy') --> 1\n",
      "('am', 'happy', 'because') --> 1\n",
      "('happy', 'because', 'i') --> 1\n",
      "('because', 'i', 'am') --> 1\n",
      "('i', 'am', 'learning') --> 1\n",
      "\n",
      "\n",
      "four\n",
      "<generator object ngrams at 0x000002096579CC78>\n",
      "('i', 'am', 'happy', 'because') --> 1\n",
      "('am', 'happy', 'because', 'i') --> 1\n",
      "('happy', 'because', 'i', 'am') --> 1\n",
      "('because', 'i', 'am', 'learning') --> 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "def cal_ngram_freq(n_gram):\n",
    "    pair_freq=defaultdict(int)\n",
    "    for pair in n_gram:\n",
    "        pair_freq[pair]+=1\n",
    "    return pair_freq\n",
    "sentence=input('enter sentence:')\n",
    "\n",
    "\n",
    "print('uni')\n",
    "n_gram=ngrams(sentence.split(),1)\n",
    "print(n_gram)\n",
    "pair_freq=cal_ngram_freq(n_gram)\n",
    "for pair,freq in pair_freq.items():\n",
    "    print(pair,'-->',freq)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('bi')\n",
    "n_gram=ngrams(sentence.split(),2)\n",
    "print(n_gram)\n",
    "pair_freq=cal_ngram_freq(n_gram)\n",
    "for pair,freq in pair_freq.items():\n",
    "    print(pair,'-->',freq)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('tri')\n",
    "n_gram=ngrams(sentence.split(),3)\n",
    "print(n_gram)\n",
    "pair_freq=cal_ngram_freq(n_gram)\n",
    "for pair,freq in pair_freq.items():\n",
    "    print(pair,'-->',freq)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('four')\n",
    "n_gram=ngrams(sentence.split(),4)\n",
    "print(n_gram)\n",
    "pair_freq=cal_ngram_freq(n_gram)\n",
    "for pair,freq in pair_freq.items():\n",
    "    print(pair,'-->',freq)\n",
    "print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a213-21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a sentence:i was i will i was i should i can i\n",
      "enter words saperated by space:i\n",
      "followinjg are probable words with probabilty:\n",
      "{'was': 0.4, 'will': 0.2, 'should': 0.2, 'can': 0.2}\n",
      "next probable word is : was\n"
     ]
    }
   ],
   "source": [
    "#next word prediction ngram model\n",
    "import nltk\n",
    "import random as ran\n",
    "from collections import defaultdict,Counter \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def pred_next_word(pre_words,model):\n",
    "    last_word=pre_words[-1]\n",
    "    if last_word not in model:\n",
    "        return None\n",
    "    next_word_prob=model[last_word]\n",
    "    \n",
    "    print('followinjg are probable words with probabilty:')\n",
    "    print(model[last_word])\n",
    "    next_word=max(next_word_prob,key=next_word_prob.get)\n",
    "    return next_word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence=input('enter a sentence:')\n",
    "tokens=nltk.word_tokenize(sentence.lower())\n",
    "bi_gram=list(nltk.bigrams(tokens))\n",
    "\n",
    "pair_freq=defaultdict(int)\n",
    "for pair in bi_gram:\n",
    "    pair_freq[pair]+=1\n",
    "\n",
    "total_word_count=Counter()\n",
    "for (w1,w2) in bi_gram:\n",
    "    total_word_count[w1]+=1\n",
    "\n",
    "model=defaultdict(dict)\n",
    "for (w1,w2),freq in pair_freq.items():\n",
    "    model[w1][w2]=freq/total_word_count[w1]\n",
    "\n",
    "    \n",
    "prev_words=input('enter words saperated by space:').lower().split()\n",
    "next_word=pred_next_word(prev_words,model)\n",
    "print(\"next probable word is :\",next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
